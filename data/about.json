{
  "properties": {
    "title": "eLife Lens: A novel way of seeing content",
    "authors": [
      "person:author-grubisic",
      "person:author-aufreiter",
      "person:author-nott",
      "person:author-hamilton",
      "person:author-mulvany"
    ]
  },
  "meta": {},
  "id": "a_new_doc",
  "nodes": {
    "heading:1": {
      "level": 1,
      "content": "Abstract",
      "id": "heading:1",
      "type": "heading"
    },
    "text:2": {
      "content": "eLife Lens provides a novel way of looking at content on the web. It is designed to make life easier for researchers, reviewers, authors and readers. For example, have you tried to look at a figure in an online article, while at the same time trying to see what the author says about the figure, jumping all around the article, losing track of what you were looking for in the first place? The reason for this is that most online research articles are published in a fixed digital version of the original paper. With eLife Lens, we take full advantage of the dynamic nature of HTML combined with javascript (Video 1).",
      "id": "text:2",
      "type": "text"
    },
    "heading:2": {
      "level": 1,
      "content": "Motivation",
      "id": "heading:2",
      "type": "heading"
    },
    "text:intro1": {
      "id": "text:intro1",
      "type": "text",
      "content": "Working with digital documents has been difficult for the most part, because they come in presentation-centric formats, optimized for print and with the intent to have the same display across multiple devices. Ultimately, the display of these documents is preserved to allow the user to print the exact same document across any device. Content today, however, is no longer being printed out readily; instead, it is read on a variety of platforms spanning computers and mobile devices. The limitations presented by different screen sizes, the lack of tactile feedback that comes from flipping between pages and inability to purely focus on the author’s arguments are problems present in all disciplines."
    },
    "text:intro2": {
      "id": "text:intro2",
      "type": "text",
      "content": "The web browser provides a unified platform for viewing content. Instead of binding the content to a presentation focused format, we can view the content as data (Aufreiter 2013). Thereby making the content readily accessible by utilizing a defined data structure. This data structure can then be processed in similar ways to databases -- allowing users to query the content, create new content and build new tools."
    },
    "text:intro3": {
      "id": "text:intro3",
      "type": "text",
      "content": "We have decided to focus our initial efforts on applying the data-centric representation of content to the scientific literature. The Open Access scientific community has standardized much of their content into a formally annotated XML format, making it easier to gain access to a large library of articles. The content of scientific articles is a combination of text, figures, tables, videos and references which are used to form the author’s argument. Scientific arguments are also rarely linear in nature, making it difficult to follow an argument when all of the content is not readily viewable on a digital device. The large amount of available data and non-linear format of the articles, provided us with the ideal start to test the flexibility of our data-centric approach. With the release of Lens, we would like to not only promote the data-centric representation of scientific content, but that of any content, which would be optimized for viewing on web clients."
    },
    "heading:3": {
      "level": 1,
      "content": "The Presentation of Lens",
      "id": "heading:3",
      "type": "heading"
    },
    "text:pres1": {
      "content": "When designing Lens, we worked to provide a flexible experience that supports a variety of use cases. On the far left the document map (Figure 2A) identifies the readers position in the context of the whole article. It also outlines each paragraph in the article. The left panel includes all of the textual content of the article (Figure 2B). The right panel includes the resources (Figure 2C). The resources are defined as the table of contents, figures, tables, videos, supplemental data, references and information about the article. By splitting apart the content into individual panels, we enable the reader to focus on multiple content bits at the same time.",
      "id": "text:pres1",
      "type": "text"
    },
    "text:pres2": {
      "content": "When reading scientific articles, it is rare for a reader to read straight through the text and figures in order. Instead, they may look to get a quick overview of the paper by looking at the figures first. By splitting the figures out from the text, the reader can transition between the two independently of one another. This allows the reader to simultaneously view exactly the content they want to focus on at that moment.",
      "id": "text:pres2",
      "type": "text"
    },
    "text:pres3": {
      "content": "Two viewing modes that are implicitly supported are text-centric and resource-centric viewing. Text-centric viewing creates a microscale reading experience by limiting the viewable content to a singular content node. When a figure or publication label is selected within a text node, only the figures or publications that are referenced within that paragraph will be displayed in their appropriate resource sections (Figure 3, Figure 4). Selection of any figure or publication will color the paragraphs in the document map (Figure 2A) that include a reference to the selected resource.",
      "id": "text:pres3",
      "type": "text"
    },
    "text:pres4": {
      "content": "The presentation of the article information has also been redesigned to organize the information in a uniform way. Each publisher will have information like the author’s impact statement, article keywords, major dataset links, etc. placed in different positions of their HTML or PDF versions of the articles. The aim is to distill all of the article information into organized subsections that can be displayed together on individual cards. The information cards create a uniform viewing experience making it easy to quickly find what the reader is looking for.",
      "id": "text:pres4",
      "type": "text"
    },
    "text:pres5": {
      "content": "In addition to simplifying the reading experience of research articles, Lens also provides useful tools to share content with others. Each of the content nodes in the data structure are deep-linked which means that each state is defined by a given URL. Sharing a specific URL will take the new reader to the exact same state in their browser which will facilitate the ease discussions about the content.",
      "id": "text:pres5",
      "type": "text"
    },

    "heading:4": {
      "level": 1,
      "content": "Open Source",
      "id": "heading:4",
      "type": "heading"
    },
    "text:5": {
      "content": "Lens is open source software. Its codebase is fairly small (less than 1000 lines of code) and available on Github. We'd like to invite anyone to use Lens for displaying their own content and get involved in the development process.",
      "id": "text:5",
      "type": "text"
    },
    "text:6": {
      "content": "Lens utilizes a number existing open source components, including Backbone to structure the application code, Substance Document, which serves as our document model and Ken for faceted filtering.",
      "id": "text:6",
      "type": "text"
    },
    "heading:10": {
      "level": 1,
      "content": "The Lens Document Format",
      "id": "heading:10",
      "type": "heading"
    },
    "text:3": {
      "content": "We are convinced that there is a need to break with traditional presentation-centric formats, by considering content as data and making it accessible in new ways (Aufreiter 2013). Therefore with the release of Lens, we’d like to promote a data-centric representation of scientific content, optimized for being consumed by web-clients.",
      "id": "text:3",
      "type": "text"
    },
    "text:12": {
      "content": "Lens can display any document that conforms to a simple JSON representation. JSON representations are flexible and can be adapted to any type of content. The initial JSON object provides the framework for organizing all of the content in an article. The id and properties keys provide global access if many JSON articles are to be queried for an initial round of filtering. The nodes key contains all of the content and the view key defines indices about what order the nodes will be displayed. The nodes are then linked together via annotations. The annotations are defined by either an explicit reference to a target content node that is contained in its source content code or font styling. Figure 1 outlines a simple example of how to use the JSON representation to compose an article for Lens. ",
      "id": "text:12",
      "type": "text"
    },
    "text:13": {
      "id": "text:13",
      "type": "text",
      "content": "The creation of an article's JSON depends on a converter script that runs through the XML tags using the sax-js node.js module. The XML tags define various types of content nodes. Each node contains type, id and content keys that the viewer then uses to define the rendering. All of the keys have their own definitions with regards to how Lens renders them. The translation from the XML to a structured JSON creates a consistent presentation scheme for the content."
    },
    "heading:9": {
      "level": 1,
      "content": "Next Steps",
      "id": "heading:9",
      "type": "heading"
    },
    "text:10": {
      "content": "This tool is an initial prototype, it was developed with the goal of getting it to market, and in your hands, as quickly as possible. The entire project went from idea to launch in just 14 weeks. Based on the user feedback we get we'll contintue to improve eLife Lens. We'd like to invite other content providers to make their content available for viewing in Lens. We also plan to support user annotations and comments. We intend to investigate integrating Lens into the peer review process.",
      "id": "text:10",
      "type": "text"
    },
    "text:11": {
      "id": "text:11",
      "type": "text",
      "content": "By utilizing a data-centric approach to content, we have created a flexible and scalable platform that can be applied to any form of content. The multiple viewing experiences presented in Lens creates an interesting opportunity for content generators. Authors now have a higher level of control with regards to how their argument is presented. In addition to having explicit references in the text, the author can include implicit resources to provide additional context to the reader. An author would also be able to query all of their older content to pull specific bits of content into their new work. In demonstrating the utility of the Lens platform with the Open Access scientific community, we are curious to see how others look to leverage the platform for generating new content."
    },
    "heading:7": {
      "level": 1,
      "content": "Credits",
      "id": "heading:7",
      "type": "heading"
    },
    "text:8": {
      "content": "eLife Lens was developed in collaboration between UC Berkeley graduate student Ivan Grubisic and eLife. He was supported by Michael Aufreiter from Substance, who helped with the design and implementation of the tool, Ian Hamilton from ripe who gave advice on the UI design and Graham Nott implemented the deployment workflow. eLife provided support for the project and the initial corpus of documents against which the tool was developed.",
      "id": "text:8",
      "type": "text"
    },
    "cover:1": {
      "id": "cover:1",
      "type": "cover"
    },
    "video:video1": {
      "type": "video",
      "id": "video:video1",
      "label": "Video 1.",
      "url_webm": "http://cdn.elifesciences.org/video/eLifeLensIntro2.webm",
      "url_ogv": "http://cdn.elifesciences.org/video/eLifeLensIntro2.ogv",
      "caption": "caption:301",
      "url": "http://cdn.elifesciences.org/video/eLifeLensIntro2.mp4",
      "poster": "http://cdn.elifesciences.org/video/eLifeLensIntro2.png"
    },
    "caption:301": {
      "type": "caption",
      "id": "caption:301",
      "title": "Introducing eLife Lens",
      "content": "Watch Ian Mulvany from eLife demonstrating Lens.",
      "source": "video:video1"
    },

    "annotation:1": {
      "type": "figure_reference",
      "id": "annotation:1",
      "source": "text:2",
      "target": "video:video1",
      "key": "content",
      "content": "Video 1",
      "pos": [
        608,
        7
      ]
    },

    "image:docformat": {
      "type": "image",
      "id": "image:docformat",
      "label": "Figure 1",
      "object_id": "F1",
      "caption": "caption:105",
      "url": "http://cdn.elifesciences.org/lens/about/aboutf001.png",
      "large_url": "http://cdn.elifesciences.org/lens/about/aboutf001.png"
    },

    "caption:105": {
      "type": "caption",
      "id": "caption:105",
      "title": "The Lens Document Format",
      "content": "An example of how to begin creating content for Lens. The backbone organizes all of the data in a readily accessible manner. The text node defines the paragraph that may reference figures or publications. The figure node defines the image that will be displayed while the annotation links the figure to the associated text node that references the figure.",
      "source": "image:docformat"
    },

    "image:lens-workspace": {
      "type": "image",
      "id": "image:lens-workspace",
      "label": "Figure 2",
      "object_id": "F2",
      "caption": "caption:106",
      "url": "http://cdn.elifesciences.org/lens/about/aboutf002.jpg",
      "large_url": "http://cdn.elifesciences.org/lens/about/aboutf002.jpg"
    },

    "caption:106": {
      "type": "caption",
      "id": "caption:106",
      "title": "The workspace of Lens",
      "content": "A. The document map. Each gray bar identifies an individual paragraph within the article. B. The article’s text content. C. All of the associated resources. This includes the Table of Contents, figures, references and additional information, including meta-data, about the article.",
      "source": "image:lens-workspace"
    },

    "annotation:25" : {
      "type" : "strong",
      "id" : "annotation:25",
      "key" : "content",
      "source" : "caption:106",
      "target" : null,
      "content" : "A.",
      "pos" : [1,2]
    },

    "annotation:26" : {
      "type" : "strong",
      "id" : "annotation:26",
      "key" : "content",
      "source" : "caption:110",
      "target" : null,
      "content" : "B.",
      "pos" : [90,2]
    },

    "annotation:27" : {
      "type" : "strong",
      "id" : "annotation:27",
      "key" : "content",
      "source" : "caption:110",
      "target" : null,
      "content" : "C.",
      "pos" : [121,2]
    },

    "image:publications": {
      "type": "image",
      "id": "image:publications",
      "label": "Figure 4",
      "object_id": "F4",
      "caption": "caption:108",
      "graphic_id": "elife00378f001",
      "url": "http://cdn.elifesciences.org/lens/about/aboutf004.png",
      "large_url": "http://cdn.elifesciences.org/lens/about/aboutf004.png"
    },

    "caption:108": {
      "type": "caption",
      "id": "caption:108",
      "title": "Focus on relevant publications",
      "content": "Clicking on a reference label reveals details about the referenced publication. The blue highlighted link symbol is a quick link to view all of the publications that are referenced within the selected paragraph.",
      "source": "image:coverpage"
    },

    "image:focusmode": {
      "type": "image",
      "id": "image:focusmode",
      "label": "Figure 3",
      "object_id": "F3",
      "caption": "caption:110",
      "graphic_id": "elife00378f001",
      "url": "http://cdn.elifesciences.org/lens/about/aboutf003.jpg",
      "large_url": "http://cdn.elifesciences.org/lens/about/aboutf003.jpg"
    },

    "caption:110": {
      "type": "caption",
      "id": "caption:110",
      "title": "Focus on relevant figures",
      "content": "Selecting a figure or reference label within the textual content, displays only the pertinent content in the resources panel. The green highlighted camera is a quick link to view all of the figures that are referenced within the selected paragraph. The black arrow to the left of the document map is a visual cue for the reader to quickly return to the paragraph they were on if they navigate away from this to another paragraph that references Figure 5. The reader can select the figure or reference label in the resources panel (Figure 2C) to focus on the paragraphs where a specific figure is mentioned. The document map (Figure 2A) will highlight the paragraphs that reference the selected resource.",
      "source": "image:focusmode"
    },

    "annotation:28" : {
      "type" : "figure_reference",
      "id" : "annotation:28",
      "key" : "content",
      "source" : "caption:110",
      "target" : "image:lens-workspace",
      "content" : "Figure 2C",
      "pos" : [531,9]
    },

    "annotation:29" : {
      "type" : "figure_reference",
      "id" : "annotation:29",
      "key" : "content",
      "source" : "caption:110",
      "target" : "image:lens-workspace",
      "content" : "Figure 2A",
      "pos" : [625,9]
    },
    
    "annotation:6": {
      "type": "figure_reference",
      "id": "annotation:6",
      "source": "text:pres1",
      "target": "image:lens-workspace",
      "key": "content",
      "pos": [
        136,
        9
      ]
    },

    "annotation:7": {
      "type": "figure_reference",
      "id": "annotation:7",
      "source": "text:pres1",
      "target": "image:lens-workspace",
      "key": "content",
      "pos": [
        383,
        9
      ]
    },

    "annotation:8": {
      "type": "figure_reference",
      "id": "annotation:8",
      "source": "text:pres1",
      "target": "image:lens-workspace",
      "key": "content",
      "pos": [
        331,
        9
      ]
    },

    "annotation:9": {
      "type": "figure_reference",
      "id": "annotation:9",
      "source": "text:12",
      "target": "image:docformat",
      "key": "content",
      "pos": [
        694,
        8
      ]
    },

    "annotation:10": {
      "type": "figure_reference",
      "id": "annotation:10",
      "source": "text:pres3",
      "target": "image:focusmode",
      "key": "content",
      "pos": [
        417,
        8
      ]
    },

    "annotation:11": {
      "type": "figure_reference",
      "id": "annotation:11",
      "source": "text:pres3",
      "target": "image:publications",
      "key": "content",
      "pos": [
        427,
        8
      ]
    },

    "annotation:12": {
      "type": "figure_reference",
      "id": "annotation:12",
      "source": "text:pres3",
      "target": "image:lens-workspace",
      "key": "content",
      "pos": [
        524,
        9
      ]
    },

    "annotation:13": {
      "type": "link",
      "id": "annotation:13",
      "source": "text:8",
      "url": "http://twitter.com/_mql",
      "key": "content",
      "pos": [
        123,
        18
      ]
    },

    "annotation:14": {
      "type": "link",
      "id": "annotation:14",
      "source": "text:8",
      "url": "http://substance.io",
      "key": "content",
      "pos": [
        147,
        9
      ]
    },

    "annotation:15": {
      "type": "link",
      "id": "annotation:15",
      "source": "text:8",
      "url": "http://colorcrate.com/",
      "key": "content",
      "pos": [
        217,
        12
      ]
    },

    "annotation:16": {
      "type": "link",
      "id": "annotation:16",
      "source": "text:8",
      "url": "http://ripe.com",
      "key": "content",
      "pos": [
        235,
        4
      ]
    },

    "annotation:17": {
      "type": "link",
      "id": "annotation:17",
      "source": "text:8",
      "url": "http://bioegrad.berkeley.edu/",
      "key": "content",
      "pos": [
        50,
        11
      ]
    },

    "annotation:18": {
      "type": "link",
      "id": "annotation:18",
      "source": "text:8",
      "url": "http://www.linkedin.com/pub/ivan-grubisic/26/353/739",
      "key": "content",
      "pos": [
        79,
        13
      ]
    },

    "annotation:20": {
      "type": "link",
      "id": "annotation:20",
      "source": "text:8",
      "url": "http://www.grahamnott.com/",
      "key": "content",
      "pos": [
        277,
        11
      ]
    },

    "annotation:21": {
      "type": "link",
      "id": "annotation:21",
      "source": "text:5",
      "url": "http://github.com/elifesciences/lens",
      "key": "content",
      "pos": [
        107,
        6
      ]
    },

    "annotation:22": {
      "type": "link",
      "id": "annotation:22",
      "source": "text:6",
      "url": "http://backbonejs.org",
      "key": "content",
      "pos": [
        66,
        8
      ]
    },
    
    "annotation:23": {
      "type": "link",
      "id": "annotation:23",
      "source": "text:6",
      "url": "http://interior.substance.io/modules/document.html",
      "key": "content",
      "pos": [
        110,
        18
      ]
    },

    "annotation:24": {
      "type": "link",
      "id": "annotation:24",
      "source": "text:6",
      "url": "http://github.com/michael/ken",
      "key": "content",
      "pos": [
        169,
        3
      ]
    },

    "annotation:100": {
      "type": "publication_reference",
      "id": "annotation:100",
      "source": "text:3",
      "target": "article:substance",
      "key": "content",
      "pos": [
        163,
        14
      ]
    },

    "person:author-aufreiter": {
      "type": "person",
      "id": "person:author-aufreiter",
      "given-names": "Michael",
      "last-name": "Aufreiter",
      "role": "author",
      "image": "https://secure.gravatar.com/avatar/d5a959d7e57daa5433fcb9f8da40be4b?s=420&d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png",
      "affiliations": [
        {
          "label": "3",
          "department": "",
          "institution": "institution:substance"
        }
      ],
      "contribution": "Design and implementation of Lens, revising the article",
      "funding": []
    },

    "person:author-nott": {
      "type": "person",
      "id": "person:author-nott",
      "given-names": "Graham",
      "last-name": "Nott",
      "role": "author",
      "image": "https://secure.gravatar.com/avatar/75b35bfe5e434bf8b19c6200f10eea4f?s=400&d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png",
      "affiliations": [
        {
          "label": "4",
          "department": "",
          "institution": "institution:elife"
        }
      ],
      "contribution": "Implementation of the deployment workflow, revising the article",
      "funding": []
    },

    "person:author-hamilton": {
      "type": "person",
      "id": "person:author-hamilton",
      "given-names": "Ian",
      "last-name": "Hamilton",
      "role": "author",
      "image": "http://cdn.elifesciences.org/lens/author/ian-hamilton.jpg",
      "affiliations": [
        {
          "label": "4",
          "department": "",
          "institution": "institution:ripe"
        }
      ],
      "contribution": "Advice on the UI Design, revising the article",
      "funding": []
    },

    "person:author-grubisic": {
      "type": "person",
      "id": "person:author-grubisic",
      "given-names": "Ivan",
      "last-name": "Grubisic",
      "image": "https://secure.gravatar.com/avatar/344c946d46895666f41e259da89d0316?s=420&d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png",
      "role": "author",
      "affiliations": [
        {
          "label": "1",
          "department": "UC Berkeley-UCSF Graduate Program in Bioengineering, California Institute of Quantitative Biosciences",
          "institution": "institution:aff6"
        },
        {
          "label": "2",
          "department": "",
          "institution": "institution:aff2"
        }
      ],
      "contribution": "Conception of the idea, drafting and revising the article",
      "funding": []
    },

    "person:author-mulvany": {
      "type": "person",
      "id": "person:author-mulvany",
      "given-names": "Ian",

      "last-name": "Mulvany",
      "image": "https://secure.gravatar.com/avatar/fccb9ef81d69152b6096ec047428ac2e?s=400&d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png",
      "role": "author",
      "affiliations": [
        {
          "label": "5",
          "department": "",
          "institution": "institution:elife"
        }
      ],
      "contribution": "Project coordination",
      "funding": []
    },

    "institution:aff6": {
      "type": "institution",
      "id": "institution:aff6",
      "name": "University of California, Berkeley",
      "city": "Berkeley",
      "country": "United States"
    },

    "institution:aff2": {
      "type": "institution",
      "id": "institution:aff2",
      "name": "Li Ka Shing Center For Biomedical and Health Sciences, CIRM Center of Excellence, University of California, Berkeley",
      "city": "Berkeley",
      "country": "United States"
    },

    "institution:elife": {
      "type": "institution",
      "id": "institution:elife",
      "name": "eLife Publications",
      "city": "Cambridge",
      "country": "UK"
    },

    "institution:ripe": {
      "type": "institution",
      "id": "institution:ripe",
      "name": "ripe",
      "city": "Washington DC",
      "country": "United States"
    },

    "institution:substance": {
      "type": "institution",
      "id": "institution:substance",
      "name": "Substance",
      "city": "Linz",
      "country": "Austria"
    },

    "institution:starglobal": {
      "type": "institution",
      "id": "institution:starglobal",
      "name": "Star Global Advanced IT Corp. Ltd.",
      "city": "Victoria",
      "country": "BC, Canada"
    },

    "article:substance": {
      "type": "website",
      "id": "article:substance",
      "authors": [
        {
          "given-names": "Michael",
          "last-name": "Aufreiter"
        }
      ],
      "title": "Content as data - Towards open digital publishing with Substance",
      "year": "2013",
      "comment": "Open Knowledge Foundation Blog",
      "citation_url": [
        "citation:592"
      ]
    },

    "citation:592": {
      "type": "citation",
      "id": "citation:592",
      "label": "Read article",
      "url": "http://blog.okfn.org/2013/01/15/content-as-data-towards-open-digital-publishing-with-substance/"
    }
  },
  "views": {
    "content": [
      "cover:1",
      "text:2",
      "heading:2",
      "text:intro1",
      "text:intro2",
      "text:intro3",
      "heading:3",
      "text:pres1",
      "text:pres2",
      "text:pres3",
      "text:pres4",
      "text:pres5",
      "heading:10",
      "text:3",
      "text:12",
      "text:13",
      "heading:4",
      "text:5",
      "text:6",
      "heading:9",
      "text:10",
      "text:11",
      "heading:7",
      "text:8"
    ],
    "figures": ["video:video1", "image:docformat", "image:lens-workspace", "image:focusmode", "image:publications"],
    "publications": ["article:substance"],
    "info": [
      "person:author-grubisic",
      "person:author-aufreiter",
      "person:author-nott",
      "person:author-hamilton",
      "person:author-mulvany"
    ]
  }
}